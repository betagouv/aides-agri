{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d1fea0",
   "metadata": {},
   "source": [
    "# Agricultural Aid Monitoring Workflow - Modular Version\n",
    "\n",
    "This notebook demonstrates the complete agricultural monitoring workflow using the modularized package structure.\n",
    "\n",
    "## Overview\n",
    "The workflow has been modularized into:\n",
    "- **`agricultural_monitoring.config`** - Configuration and settings\n",
    "- **`agricultural_monitoring.models`** - Data models and schemas\n",
    "- **`agricultural_monitoring.extractors`** - Web content extraction\n",
    "- **`agricultural_monitoring.processors`** - LLM processing and filtering\n",
    "- **`agricultural_monitoring.workflows`** - Complete monitoring workflows\n",
    "- **`agricultural_monitoring.monitoring`** - LangSmith observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac4274",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58628c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theo.moreau/Documents/betagouv/aides-agri/.venv/lib/python3.13/site-packages/pydantic/_internal/_config.py:383: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modular agricultural monitoring package loaded\n",
      "‚úÖ Langfuse tracing: enabled\n",
      "‚úÖ Target URLs: 3 configured\n",
      "   1. https://agriculture.gouv.fr/mots-cles/aides\n",
      "   2. https://ain-rhone.msa.fr/lfp/soutien-exploitant\n",
      "   3. https://www.franceagrimer.fr/rechercher-une-aide\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from agricultural_monitoring.config.settings import TARGET_URLS, setup_langfuse\n",
    "from agricultural_monitoring.monitoring import run_monitored_workflow, LangfuseAnalyzer\n",
    "\n",
    "# Setup environment\n",
    "langfuse_enabled = setup_langfuse()\n",
    "\n",
    "print(\"‚úÖ Modular agricultural monitoring package loaded\")\n",
    "print(f\"‚úÖ Langfuse tracing: {'enabled' if langfuse_enabled else 'disabled'}\")\n",
    "print(f\"‚úÖ Target URLs: {len(TARGET_URLS)} configured\")\n",
    "\n",
    "# Display target URLs\n",
    "for i, url in enumerate(TARGET_URLS, 1):\n",
    "    print(f\"   {i}. {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3013f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_prompt = \"\"\"\n",
    "You are an expert evaluator. You will be given two lists of articles. Each article is represented by a title (and optionally, a short description). Your task is to compare the first list to the second list, and determine the number of articles in list 1 that actually appear in list 2.\n",
    "\n",
    "Rules:\n",
    "\t1.\tIgnore differences in punctuation or minor whitespace.\n",
    "\t2.\tDo not count duplicates multiple times ‚Äî each article should be counted at most once.\n",
    "\t3.\tReturn only the number of matching articles as an integer.\n",
    "    4.  source_url has to be exact match, the rest of atributes have to be close but not exactly the same.\n",
    "\n",
    "    Score is an integer greater or equal to 0. It represents the number of articles in list 1 that actually appear in list 2 (PRECISION).\n",
    "    Give one sentence reasoning to explain the score.\n",
    "\n",
    "    Input Format:\n",
    "    {\n",
    "        \"list_1\": [\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            },\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    {\n",
    "        \"list_2\": [\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            },\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    Return the following format:\n",
    "    {\n",
    "        'score': '<PRECISION>',\n",
    "        'explanation': '<brief_explanation_of_how_the_score_was_determined>'\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "recall_prompt = \"\"\"\n",
    "You are an expert evaluator. You will be given two lists of articles. Each article is represented by a title (and optionally, a short description). Your task is to compare the first list to the second list, and determine how many articles from list 2 don't appear in list 1.\n",
    "\n",
    "Rules:\n",
    "\t1.\tIgnore differences in punctuation or minor whitespace.\n",
    "\t2.\tDo not count duplicates multiple times ‚Äî each article should be counted at most once.\n",
    "\t3.\tReturn only the number of matching articles as an integer.\n",
    "    4.  source_url has to be exact match, the rest of atributes have to be close but not exactly the same.\n",
    "\n",
    "    Score is an integer greater or equal to 0. It represents the number of articles articles from list 2 don't appear in list 1 (RECALL).\n",
    "    Give one sentence reasoning to explain the score.\n",
    "\n",
    "    Input Format:\n",
    "    {\n",
    "        \"list_1\": [\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            },\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    {\n",
    "        \"list_2\": [\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            },\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    Return the following format:\n",
    "    {\n",
    "        'score': '<RECALL>',\n",
    "        'explanation': '<brief_explanation_of_how_the_score_was_determined>'\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba27aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def pydantic_to_json_schema(model: Type[BaseModel], schema_name: str = \"schema\", strict: bool = True) -> dict:\n",
    "  json_schema = model.model_json_schema()\n",
    "  no_ref_json_schema = resolve_refs(json_schema)\n",
    "  formatted_json_schema = {\n",
    "    \"name\": schema_name, \n",
    "    \"strict\": strict, \n",
    "    \"schema\": {\n",
    "      \"type\": \"object\", \n",
    "      \"properties\": no_ref_json_schema['properties'],\n",
    "      \"required\": no_ref_json_schema.get(\"required\", [])\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return formatted_json_schema\n",
    "\n",
    "\n",
    "def resolve_refs(schema: dict) -> dict:\n",
    "  \"\"\"\n",
    "  By default, pydantic creates $refs and $defs as constants outside of the main schema.\n",
    "  Those $ attributes serve as reference in the main schema : write {'properties': {$def: $ref}}.\n",
    "\n",
    "  This format is supported for json validation, but not for structured output APIs.\n",
    "  This function replaces the $ref by the schema defined by $def outside the main schema.\n",
    "  \"\"\"\n",
    "  defs = schema.pop(\"$defs\", {})\n",
    "  def resolve(obj: dict) -> dict:\n",
    "      if isinstance(obj, dict):\n",
    "          if \"$ref\" in obj:\n",
    "              ref = obj[\"$ref\"].split(\"/\")[-1]\n",
    "              return resolve(defs[ref])\n",
    "          return {k: resolve(v) for k, v in obj.items()}\n",
    "      elif isinstance(obj, list):\n",
    "          return [resolve(v) for v in obj]\n",
    "      else:\n",
    "          return obj\n",
    "  return resolve(schema)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Type, Dict, Any\n",
    "\n",
    "class ChatAlbert():\n",
    "\n",
    "  def __init__(self, pydantic_schema: Type[BaseModel]) -> None:\n",
    "    load_dotenv()\n",
    "    self.api_key = os.getenv(\"ALBERT_API_KEY\")\n",
    "    self.endpoint = \"https://albert.api.etalab.gouv.fr/v1/chat/completions\"\n",
    "    self.json_schema = pydantic_to_json_schema(pydantic_schema)\n",
    "\n",
    "  def get_header(self):\n",
    "      headers = {\n",
    "          \"accept\": \"application/json\",\n",
    "          \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "          \"Content-Type\": \"application/json\",\n",
    "      }\n",
    "      return headers\n",
    "\n",
    "  def get_body(self, model_name, system_prompt, user_message, temperature: float, **kwargs) -> dict:\n",
    "      payload = {\n",
    "          \"model\": model_name,\n",
    "          \"messages\": [\n",
    "              {\n",
    "                  \"role\": \"system\",\n",
    "                  \"content\": system_prompt\n",
    "              },\n",
    "              {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": user_message\n",
    "              }\n",
    "          ],\n",
    "          \"temperature\": temperature,\n",
    "          \"response_format\": {\n",
    "              \"type\": \"json_schema\",\n",
    "              \"json_schema\": self.json_schema\n",
    "          },\n",
    "          **kwargs\n",
    "      }\n",
    "\n",
    "      return payload\n",
    "\n",
    "  def completions(self, model_name: str, system_prompt: str, user_message: str, temperature: float = 0.2, **kwargs) -> str:\n",
    "      import requests\n",
    "      headers = self.get_header()\n",
    "      body = self.get_body(model_name, system_prompt, user_message, temperature, **kwargs)\n",
    "\n",
    "      response = requests.post(self.endpoint, headers=headers, json=body)\n",
    "      response.raise_for_status()\n",
    "      json_response = response.json()\n",
    "      return json_response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3419dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import inspect\n",
    "\n",
    "from langfuse import get_client\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langfuse import Evaluation\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from agricultural_monitoring import HybridWorkflow, AgentWorkflow, LLMWorkflow\n",
    "\n",
    "class MetricResult(BaseModel):\n",
    "    explanation: str = Field(..., description=\"Explanation of the metric result\")\n",
    "    score: float = Field(..., description=\"Score of the metric result\")\n",
    "\n",
    "load_dotenv()\n",
    "_langfuse_client = get_client()\n",
    "\n",
    "def list_articles_llm(*, item, **kwargs):\n",
    "    url = item.input[\"url\"]\n",
    "    workflow = LLMWorkflow()\n",
    "    # Wrap sync function in an awaitable for uniformity\n",
    "    result = workflow.monitor_url(url)\n",
    "    return result.get(\"normalized_data\", {})\n",
    "\n",
    "async def list_articles_hybrid(*, item, **kwargs):\n",
    "    url = item.input[\"url\"]\n",
    "    workflow = HybridWorkflow()\n",
    "    # Wrap sync function in an awaitable for uniformity\n",
    "    result = await workflow.monitor_url(url)\n",
    "    return result.get(\"normalized_data\", {})\n",
    "\n",
    "async def list_articles_agent(*, item, **kwargs):\n",
    "    url = item.input[\"url\"]\n",
    "    workflow = AgentWorkflow()\n",
    "    # Wrap sync function in an awaitable for uniformity\n",
    "    result = await workflow.monitor_url(url)\n",
    "    return result.get(\"llm_extraction\", {})\n",
    "\n",
    "#list_tasks = (list_articles_llm, list_articles_hybrid, list_articles_agent)\n",
    "\n",
    "def make_validate_filtered_task(workflow_cls):\n",
    "    async def task(*, item, **kwargs):\n",
    "        url = item.input[\"url\"]\n",
    "        workflow = workflow_cls()\n",
    "        # Determine if monitor_url is async\n",
    "        if inspect.iscoroutinefunction(workflow.monitor_url):\n",
    "            result = await workflow.monitor_url(url)\n",
    "        else:\n",
    "            # Wrap sync function in an awaitable for uniformity\n",
    "            result = workflow.monitor_url(url)\n",
    "        found = result.get(\"memory_filtered_data\", {})\n",
    "        filtered_articles = found.get(\"aides\", [])\n",
    "        return {\"output\": filtered_articles}\n",
    "    task.__name__ = f\"validate_filtered_articles_{workflow_cls.__name__}\"\n",
    "    return task\n",
    "\n",
    "def make_validate_filtered_llm(*, item, **kwargs):\n",
    "    url = item.input[\"url\"]\n",
    "    workflow = LLMWorkflow()\n",
    "    # Wrap sync function in an awaitable for uniformity\n",
    "    result = workflow.monitor_url(url)\n",
    "    found = result.get(\"memory_filtered_data\", {})\n",
    "    filtered_articles = found.get(\"aides\", [])\n",
    "    return {\"output\": filtered_articles}\n",
    "\n",
    "async def make_validate_filtered_hybrid(*, item, **kwargs):\n",
    "    url = item.input[\"url\"]\n",
    "    workflow = HybridWorkflow()\n",
    "    # Wrap sync function in an awaitable for uniformity\n",
    "    result = await workflow.monitor_url(url)\n",
    "    found = result.get(\"memory_filtered_data\", {})\n",
    "    filtered_articles = found.get(\"aides\", [])\n",
    "    return {\"output\": filtered_articles}\n",
    "\n",
    "async def make_validate_filtered_agent(*, item, **kwargs):\n",
    "    url = item.input[\"url\"]\n",
    "    workflow = AgentWorkflow()\n",
    "    # Wrap sync function in an awaitable for uniformity\n",
    "    result = await workflow.monitor_url(url)\n",
    "    found = result.get(\"memory_filtered_data\", {})\n",
    "    filtered_articles = found.get(\"aides\", [])\n",
    "    return {\"output\": filtered_articles}\n",
    "\n",
    "list_tasks = (make_validate_filtered_llm, make_validate_filtered_hybrid, make_validate_filtered_agent)\n",
    "\n",
    "def recall_evaluator(*, input, output, expected_output, **kwargs):\n",
    "    chat_albert = ChatAlbert(MetricResult)\n",
    "    chat_response = chat_albert.completions(\n",
    "        model_name=\"albert-large\",\n",
    "        system_prompt=recall_prompt,\n",
    "        user_message=json.dumps(output) + \"\\n\" + json.dumps(expected_output),\n",
    "        temperature=0\n",
    "    )\n",
    "    try:\n",
    "        json_result = json.loads(chat_response)\n",
    "        recall = 1 - (json_result[\"score\"] / len(expected_output[\"articles_trouves\"])) if len(expected_output[\"articles_trouves\"]) > 0 else 0\n",
    "    except json.JSONDecodeError:\n",
    "        json_result = {\"explanation\": \"oui oui baguette\", \"score\": 0}\n",
    "    return Evaluation(name=\"recall\", comment=json_result[\"explanation\"], value=recall)\n",
    "\n",
    "def precision_evaluator(*, input, output, expected_output, **kwargs):\n",
    "    chat_albert = ChatAlbert(MetricResult)\n",
    "    chat_response = chat_albert.completions(\n",
    "        model_name=\"albert-large\",\n",
    "        system_prompt=precision_prompt,\n",
    "        user_message=json.dumps(output) + \"\\n\" + json.dumps(expected_output),\n",
    "        temperature=0\n",
    "    )\n",
    "    try:\n",
    "        json_result = json.loads(chat_response)\n",
    "        precision = json_result[\"score\"] / len(output) if len(output) > 0 else 0\n",
    "    except json.JSONDecodeError:\n",
    "        json_result = {\"explanation\": \"oui oui baguette\", \"score\": 0}\n",
    "    return Evaluation(name=\"precision\", comment=json_result[\"explanation\"], value=precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for workflow: make_validate_filtered_llm\n",
      "üîç Starting monitoring workflow for: https://www.bretagne.bzh/aides/?mot-clef=&profil=entreprises-et-professionnels&cloture=0&showall=0\n",
      "üì• Fetching content from: https://www.bretagne.bzh/aides/?mot-clef=&profil=entreprises-et-professionnels&cloture=0&showall=0 (attempt 1)\n",
      "üîç Starting monitoring workflow for: https://www.morbihan.fr/aides-et-services/rechercher-une-aide\n",
      "üì• Fetching content from: https://www.morbihan.fr/aides-et-services/rechercher-une-aide (attempt 1)\n",
      "üîç Starting monitoring workflow for: https://www.auvergnerhonealpes.fr/aides?f%5B0%5D=profil%3A3\n",
      "üì• Fetching content from: https://www.auvergnerhonealpes.fr/aides?f%5B0%5D=profil%3A3 (attempt 1)\n",
      "üîç Starting monitoring workflow for: https://ain-rhone.msa.fr/lfp/soutien-exploitant\n",
      "üì• Fetching content from: https://ain-rhone.msa.fr/lfp/soutien-exploitant (attempt 1)\n",
      "üîç Starting monitoring workflow for: https://agriculture.gouv.fr/mots-cles/aides\n",
      "üì• Fetching content from: https://agriculture.gouv.fr/mots-cles/aides (attempt 1)\n"
     ]
    }
   ],
   "source": [
    "# Run 1\n",
    "dataset = _langfuse_client.get_dataset(\"filtered-article-dataset\")\n",
    "for task in list_tasks:\n",
    "    print(f\"Running experiment for workflow: {task.__name__}\")\n",
    "    result = dataset.run_experiment(\n",
    "        name=\"List article experiment with \" + task.__name__,\n",
    "        description=f\"Testing {task.__name__} for article listing task\",\n",
    "        task=task,\n",
    "        evaluators=[recall_evaluator, precision_evaluator],\n",
    "        max_concurrency=5\n",
    "    )\n",
    "\n",
    "print(result.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc88d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agricultural_monitoring.workflows.agent_workflow import WebAgentStandaloneProcessor\n",
    "\n",
    "web_agent_processor = WebAgentStandaloneProcessor()\n",
    "result = await web_agent_processor.process_content(\"https://www.franceagrimer.fr/rechercher-une-aide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"aides_identifiees\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agricultural_monitoring.workflows.agent_workflow import DataNormalizer\n",
    "\n",
    "data_norm = DataNormalizer()\n",
    "new_result = data_norm.normalize_data(result, \"https://www.franceagrimer.fr/rechercher-une-aide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f3ef7",
   "metadata": {},
   "source": [
    "## 2. Quick Start - Complete Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced workflow with inline link processing\n",
    "workflow = EnhancedAgriculturalMonitoringWorkflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc68b6f",
   "metadata": {},
   "source": [
    "## 3. Test Single URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with one URL first\n",
    "test_url = TARGET_URLS[2]\n",
    "print(f\"üß™ Testing workflow with: {test_url}\")\n",
    "\n",
    "result = await workflow.monitor_url(test_url)\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"Web extraction: {result['web_content']['status']}\")\n",
    "print(f\"LLM processing: {result['llm_extraction']['status']}\")\n",
    "print(f\"Data normalization: {result['normalized_data']['metadata']['status']}\")\n",
    "print(f\"Memory filtering: {result.get('memory_filtered_data', {}).get('status', 'N/A')}\")\n",
    "\n",
    "# Show found aids\n",
    "memory_data = result.get('memory_filtered_data', {})\n",
    "if memory_data.get('status') == 'success':\n",
    "    aids_found = memory_data.get('aides', [])\n",
    "    before_filtering = len(result['normalized_data']['aides'])\n",
    "    after_filtering = len(aids_found)\n",
    "    \n",
    "    print(f\"\\nüîç Filtering results:\")\n",
    "    print(f\"   Before filtering: {before_filtering} aids\")\n",
    "    print(f\"   After filtering: {after_filtering} aids\")\n",
    "    print(f\"   Filtered out: {memory_data.get('filtered_count', 0)} aids\")\n",
    "    \n",
    "    if aids_found:\n",
    "        print(f\"\\n‚úÖ New agricultural aids found:\")\n",
    "        for i, aide in enumerate(aids_found[:3], 1):\n",
    "            print(f\"   {i}. {aide['titre_aide']}\")\n",
    "            print(f\"      {aide['description'][:100]}...\")\n",
    "    else:\n",
    "        print(f\"\\nüìù No new aids found (all filtered by memory)\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Workflow failed at some stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9747f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset.items:\n",
    "    print(item.input, item.expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset.items:\n",
    "    print(item.input, item.expected_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb7288",
   "metadata": {},
   "source": [
    "## 4. Production Monitoring - All URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcaf283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run production monitoring with all URLs\n",
    "print(\"üöÄ Running production monitoring with all target URLs...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the monitoring function for comprehensive tracking\n",
    "results = run_monitored_workflow(workflow, TARGET_URLS)\n",
    "\n",
    "# Detailed results analysis\n",
    "successful_results = results['successful_results']\n",
    "failed_results = results['failed_results']\n",
    "summary = results['summary']\n",
    "\n",
    "print(f\"\\nüìã DETAILED RESULTS:\")\n",
    "for i, result in enumerate(successful_results + failed_results, 1):\n",
    "    url = result.get('web_content', {}).get('url', 'Unknown')\n",
    "    status = result.get('normalized_data', {}).get('metadata', {}).get('status', 'Unknown')\n",
    "    \n",
    "    print(f\"\\n{i}. {url}\")\n",
    "    print(f\"   Status: {status}\")\n",
    "    \n",
    "    if status == 'success':\n",
    "        aids_before = len(result['normalized_data']['aides'])\n",
    "        memory_data = result.get('memory_filtered_data', {})\n",
    "        aids_after = memory_data.get('new_count', aids_before) if memory_data.get('status') == 'success' else aids_before\n",
    "        \n",
    "        print(f\"   Web extraction: ‚úÖ\")\n",
    "        print(f\"   Content length: {len(result['web_content']['content']):,} chars\")\n",
    "        print(f\"   Links found: {result['web_content'].get('links_count', 0)}\")\n",
    "        print(f\"   Aids before filtering: {aids_before}\")\n",
    "        print(f\"   Aids after filtering: {aids_after}\")\n",
    "        \n",
    "        # Show sample aids\n",
    "        final_aids = memory_data.get('aides', result['normalized_data']['aides']) if memory_data.get('status') == 'success' else result['normalized_data']['aides']\n",
    "        if final_aids:\n",
    "            print(f\"   Sample aids:\")\n",
    "            for aide in final_aids[:2]:\n",
    "                print(f\"     ‚Ä¢ {aide['titre_aide']}\")\n",
    "    else:\n",
    "        error = result.get('normalized_data', {}).get('metadata', {}).get('error', 'Unknown error')\n",
    "        print(f\"   Error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687fb77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agricultural_monitoring.extractors import EnhancedWebContentExtractor\n",
    "from agricultural_monitoring.processors import LLMProcessor, WebAgentProcessor\n",
    "from agricultural_monitoring.processors import DataNormalizer\n",
    "\n",
    "URL = \"https://www.bretagne.bzh/aides/?mot-clef=&profil=entreprises-et-professionnels&cloture=0&showall=0\"\n",
    "\n",
    "web_extractor = EnhancedWebContentExtractor()\n",
    "llm_processor = LLMProcessor()\n",
    "web_agent_processor = WebAgentProcessor()\n",
    "normalizer = DataNormalizer()\n",
    "\n",
    "# Step 2: Process the extracted content\n",
    "processed_content = await web_agent_processor.process_content(URL)\n",
    "\n",
    "# Step 3: Normalize the processed content\n",
    "normalized_content = normalizer.normalize_data(processed_content, URL)\n",
    "\n",
    "print(processed_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"test.json\", \"w\") as f:\n",
    "    json.dump(normalized_content, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d5d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# 1. Take screenshot\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.franceagrimer.fr/rechercher-une-aide\")\n",
    "driver.save_screenshot(\"page.png\")\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aides-agri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
