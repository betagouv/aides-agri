{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d1fea0",
   "metadata": {},
   "source": [
    "# Agricultural Aid Monitoring Workflow - Modular Version\n",
    "\n",
    "This notebook demonstrates the complete agricultural monitoring workflow using the modularized package structure.\n",
    "\n",
    "## Overview\n",
    "The workflow has been modularized into:\n",
    "- **`agricultural_monitoring.config`** - Configuration and settings\n",
    "- **`agricultural_monitoring.models`** - Data models and schemas\n",
    "- **`agricultural_monitoring.extractors`** - Web content extraction\n",
    "- **`agricultural_monitoring.processors`** - LLM processing and filtering\n",
    "- **`agricultural_monitoring.workflows`** - Complete monitoring workflows\n",
    "- **`agricultural_monitoring.monitoring`** - LangSmith observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac4274",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58628c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theo.moreau/Documents/betagouv/aides-agri/.venv/lib/python3.13/site-packages/pydantic/_internal/_config.py:383: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modular agricultural monitoring package loaded\n",
      "✅ Langfuse tracing: enabled\n",
      "✅ Target URLs: 3 configured\n",
      "   1. https://agriculture.gouv.fr/mots-cles/aides\n",
      "   2. https://ain-rhone.msa.fr/lfp/soutien-exploitant\n",
      "   3. https://www.franceagrimer.fr/rechercher-une-aide\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from agricultural_monitoring.config.settings import TARGET_URLS, setup_langfuse\n",
    "from agricultural_monitoring.monitoring import run_monitored_workflow, LangfuseAnalyzer\n",
    "\n",
    "# Setup environment\n",
    "langfuse_enabled = setup_langfuse()\n",
    "\n",
    "print(\"✅ Modular agricultural monitoring package loaded\")\n",
    "print(f\"✅ Langfuse tracing: {'enabled' if langfuse_enabled else 'disabled'}\")\n",
    "print(f\"✅ Target URLs: {len(TARGET_URLS)} configured\")\n",
    "\n",
    "# Display target URLs\n",
    "for i, url in enumerate(TARGET_URLS, 1):\n",
    "    print(f\"   {i}. {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3013f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_prompt = \"\"\"\n",
    "You are an expert evaluator. You will be given two lists of articles. Each article is represented by a title (and optionally, a short description). Your task is to compare the first list to the second list, and determine the number of articles in list 1 that actually appear in list 2.\n",
    "\n",
    "Rules:\n",
    "\t1.\tIgnore differences in punctuation or minor whitespace.\n",
    "\t2.\tDo not count duplicates multiple times — each article should be counted at most once.\n",
    "\t3.\tReturn only the number of matching articles as an integer.\n",
    "    4.  source_url has to be exact match, the rest of atributes have to be close but not exactly the same.\n",
    "\n",
    "    Score is an integer greater or equal to 0. It represents the number of articles in list 1 that actually appear in list 2 (PRECISION).\n",
    "    Give one sentence reasoning to explain the score.\n",
    "\n",
    "    Input Format:\n",
    "    {\n",
    "        \"list_1\": [\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            },\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    {\n",
    "        \"list_2\": [\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            },\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    Return the following format:\n",
    "    {\n",
    "        'score': '<PRECISION>',\n",
    "        'explanation': '<brief_explanation_of_how_the_score_was_determined>'\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "recall_prompt = \"\"\"\n",
    "You are an expert evaluator. You will be given two lists of articles. Each article is represented by a title (and optionally, a short description). Your task is to compare the first list to the second list, and determine how many articles from list 2 don't appear in list 1.\n",
    "\n",
    "Rules:\n",
    "\t1.\tIgnore differences in punctuation or minor whitespace.\n",
    "\t2.\tDo not count duplicates multiple times — each article should be counted at most once.\n",
    "\t3.\tReturn only the number of matching articles as an integer.\n",
    "    4.  source_url has to be exact match, the rest of atributes have to be close but not exactly the same.\n",
    "\n",
    "    Score is an integer greater or equal to 0. It represents the number of articles articles from list 2 don't appear in list 1 (RECALL).\n",
    "    Give one sentence reasoning to explain the score.\n",
    "\n",
    "    Input Format:\n",
    "    {\n",
    "        \"list_1\": [\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            },\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    {\n",
    "        \"list_2\": [\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            },\n",
    "            {\n",
    "                \"title\": <string>,\n",
    "                \"description\": <string>,\n",
    "                \"source_url\": <string>\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    Return the following format:\n",
    "    {\n",
    "        'score': '<RECALL>',\n",
    "        'explanation': '<brief_explanation_of_how_the_score_was_determined>'\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba27aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def pydantic_to_json_schema(model: Type[BaseModel], schema_name: str = \"schema\", strict: bool = True) -> dict:\n",
    "  json_schema = model.model_json_schema()\n",
    "  no_ref_json_schema = resolve_refs(json_schema)\n",
    "  formatted_json_schema = {\n",
    "    \"name\": schema_name, \n",
    "    \"strict\": strict, \n",
    "    \"schema\": {\n",
    "      \"type\": \"object\", \n",
    "      \"properties\": no_ref_json_schema['properties'],\n",
    "      \"required\": no_ref_json_schema.get(\"required\", [])\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return formatted_json_schema\n",
    "\n",
    "\n",
    "def resolve_refs(schema: dict) -> dict:\n",
    "  \"\"\"\n",
    "  By default, pydantic creates $refs and $defs as constants outside of the main schema.\n",
    "  Those $ attributes serve as reference in the main schema : write {'properties': {$def: $ref}}.\n",
    "\n",
    "  This format is supported for json validation, but not for structured output APIs.\n",
    "  This function replaces the $ref by the schema defined by $def outside the main schema.\n",
    "  \"\"\"\n",
    "  defs = schema.pop(\"$defs\", {})\n",
    "  def resolve(obj: dict) -> dict:\n",
    "      if isinstance(obj, dict):\n",
    "          if \"$ref\" in obj:\n",
    "              ref = obj[\"$ref\"].split(\"/\")[-1]\n",
    "              return resolve(defs[ref])\n",
    "          return {k: resolve(v) for k, v in obj.items()}\n",
    "      elif isinstance(obj, list):\n",
    "          return [resolve(v) for v in obj]\n",
    "      else:\n",
    "          return obj\n",
    "  return resolve(schema)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Type, Dict, Any\n",
    "\n",
    "class ChatAlbert():\n",
    "\n",
    "  def __init__(self, pydantic_schema: Type[BaseModel]) -> None:\n",
    "    load_dotenv()\n",
    "    self.api_key = os.getenv(\"ALBERT_API_KEY\")\n",
    "    self.endpoint = \"https://albert.api.etalab.gouv.fr/v1/chat/completions\"\n",
    "    self.json_schema = pydantic_to_json_schema(pydantic_schema)\n",
    "\n",
    "  def get_header(self):\n",
    "      headers = {\n",
    "          \"accept\": \"application/json\",\n",
    "          \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "          \"Content-Type\": \"application/json\",\n",
    "      }\n",
    "      return headers\n",
    "\n",
    "  def get_body(self, model_name, system_prompt, user_message, temperature: float, **kwargs) -> dict:\n",
    "      payload = {\n",
    "          \"model\": model_name,\n",
    "          \"messages\": [\n",
    "              {\n",
    "                  \"role\": \"system\",\n",
    "                  \"content\": system_prompt\n",
    "              },\n",
    "              {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": user_message\n",
    "              }\n",
    "          ],\n",
    "          \"temperature\": temperature,\n",
    "          \"response_format\": {\n",
    "              \"type\": \"json_schema\",\n",
    "              \"json_schema\": self.json_schema\n",
    "          },\n",
    "          **kwargs\n",
    "      }\n",
    "\n",
    "      return payload\n",
    "\n",
    "  def completions(self, model_name: str, system_prompt: str, user_message: str, temperature: float = 0.2, **kwargs) -> str:\n",
    "      import requests\n",
    "      headers = self.get_header()\n",
    "      body = self.get_body(model_name, system_prompt, user_message, temperature, **kwargs)\n",
    "\n",
    "      response = requests.post(self.endpoint, headers=headers, json=body)\n",
    "      response.raise_for_status()\n",
    "      json_response = response.json()\n",
    "      return json_response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3419dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import inspect\n",
    "\n",
    "from langfuse import get_client\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langfuse import Evaluation\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from agricultural_monitoring import HybridWorkflow, AgentWorkflow, LLMWorkflow\n",
    "\n",
    "class MetricResult(BaseModel):\n",
    "    explanation: str = Field(..., description=\"Explanation of the metric result\")\n",
    "    score: float = Field(..., description=\"Score of the metric result\")\n",
    "\n",
    "load_dotenv()\n",
    "_langfuse_client = get_client()\n",
    "\n",
    "def list_articles_llm(*, item, **kwargs):\n",
    "    url = item.input[\"url\"]\n",
    "    workflow = LLMWorkflow()\n",
    "    # Wrap sync function in an awaitable for uniformity\n",
    "    result = workflow.monitor_url(url)\n",
    "    return result.get(\"normalized_data\", {})\n",
    "\n",
    "async def list_articles_hybrid(*, item, **kwargs):\n",
    "    url = item.input[\"url\"]\n",
    "    workflow = HybridWorkflow()\n",
    "    # Wrap sync function in an awaitable for uniformity\n",
    "    result = await workflow.monitor_url(url)\n",
    "    return result.get(\"normalized_data\", {})\n",
    "\n",
    "async def list_articles_agent(*, item, **kwargs):\n",
    "    url = item.input[\"url\"]\n",
    "    workflow = AgentWorkflow()\n",
    "    # Wrap sync function in an awaitable for uniformity\n",
    "    result = await workflow.monitor_url(url)\n",
    "    return result.get(\"llm_extraction\", {})\n",
    "\n",
    "#list_tasks = (list_articles_llm, list_articles_hybrid, list_articles_agent)\n",
    "\n",
    "def make_validate_filtered_task(workflow_cls):\n",
    "    async def task(*, item, **kwargs):\n",
    "        url = item.input[\"url\"]\n",
    "        workflow = workflow_cls()\n",
    "        # Determine if monitor_url is async\n",
    "        if inspect.iscoroutinefunction(workflow.monitor_url):\n",
    "            result = await workflow.monitor_url(url)\n",
    "        else:\n",
    "            # Wrap sync function in an awaitable for uniformity\n",
    "            result = workflow.monitor_url(url)\n",
    "        found = result.get(\"memory_filtered_data\", {})\n",
    "        filtered_articles = found.get(\"aides\", [])\n",
    "        return {\"output\": filtered_articles}\n",
    "    task.__name__ = f\"validate_filtered_articles_{workflow_cls.__name__}\"\n",
    "    return task\n",
    "\n",
    "def make_validate_filtered_llm(*, item, **kwargs):\n",
    "    url = item.input[\"url\"]\n",
    "    workflow = LLMWorkflow()\n",
    "    # Wrap sync function in an awaitable for uniformity\n",
    "    result = workflow.monitor_url(url)\n",
    "    found = result.get(\"memory_filtered_data\", {})\n",
    "    filtered_articles = found.get(\"aides\", [])\n",
    "    return {\"output\": filtered_articles}\n",
    "\n",
    "async def make_validate_filtered_hybrid(*, item, **kwargs):\n",
    "    url = item.input[\"url\"]\n",
    "    workflow = HybridWorkflow()\n",
    "    # Wrap sync function in an awaitable for uniformity\n",
    "    result = await workflow.monitor_url(url)\n",
    "    found = result.get(\"memory_filtered_data\", {})\n",
    "    filtered_articles = found.get(\"aides\", [])\n",
    "    return {\"output\": filtered_articles}\n",
    "\n",
    "async def make_validate_filtered_agent(*, item, **kwargs):\n",
    "    url = item.input[\"url\"]\n",
    "    workflow = AgentWorkflow()\n",
    "    # Wrap sync function in an awaitable for uniformity\n",
    "    result = await workflow.monitor_url(url)\n",
    "    found = result.get(\"memory_filtered_data\", {})\n",
    "    filtered_articles = found.get(\"aides\", [])\n",
    "    return {\"output\": filtered_articles}\n",
    "\n",
    "list_tasks = (make_validate_filtered_llm, make_validate_filtered_hybrid, make_validate_filtered_agent)\n",
    "\n",
    "def recall_evaluator(*, input, output, expected_output, **kwargs):\n",
    "    chat_albert = ChatAlbert(MetricResult)\n",
    "    chat_response = chat_albert.completions(\n",
    "        model_name=\"albert-large\",\n",
    "        system_prompt=recall_prompt,\n",
    "        user_message=json.dumps(output) + \"\\n\" + json.dumps(expected_output),\n",
    "        temperature=0\n",
    "    )\n",
    "    try:\n",
    "        json_result = json.loads(chat_response)\n",
    "        recall = 1 - (json_result[\"score\"] / len(expected_output[\"articles_trouves\"])) if len(expected_output[\"articles_trouves\"]) > 0 else 0\n",
    "    except json.JSONDecodeError:\n",
    "        json_result = {\"explanation\": \"oui oui baguette\", \"score\": 0}\n",
    "    return Evaluation(name=\"recall\", comment=json_result[\"explanation\"], value=recall)\n",
    "\n",
    "def precision_evaluator(*, input, output, expected_output, **kwargs):\n",
    "    chat_albert = ChatAlbert(MetricResult)\n",
    "    chat_response = chat_albert.completions(\n",
    "        model_name=\"albert-large\",\n",
    "        system_prompt=precision_prompt,\n",
    "        user_message=json.dumps(output) + \"\\n\" + json.dumps(expected_output),\n",
    "        temperature=0\n",
    "    )\n",
    "    try:\n",
    "        json_result = json.loads(chat_response)\n",
    "        precision = json_result[\"score\"] / len(output) if len(output) > 0 else 0\n",
    "    except json.JSONDecodeError:\n",
    "        json_result = {\"explanation\": \"oui oui baguette\", \"score\": 0}\n",
    "    return Evaluation(name=\"precision\", comment=json_result[\"explanation\"], value=precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for workflow: make_validate_filtered_llm\n",
      "🔍 Starting monitoring workflow for: https://www.bretagne.bzh/aides/?mot-clef=&profil=entreprises-et-professionnels&cloture=0&showall=0\n",
      "📥 Fetching content from: https://www.bretagne.bzh/aides/?mot-clef=&profil=entreprises-et-professionnels&cloture=0&showall=0 (attempt 1)\n",
      "🔍 Starting monitoring workflow for: https://www.morbihan.fr/aides-et-services/rechercher-une-aide\n",
      "📥 Fetching content from: https://www.morbihan.fr/aides-et-services/rechercher-une-aide (attempt 1)\n",
      "🔍 Starting monitoring workflow for: https://www.auvergnerhonealpes.fr/aides?f%5B0%5D=profil%3A3\n",
      "📥 Fetching content from: https://www.auvergnerhonealpes.fr/aides?f%5B0%5D=profil%3A3 (attempt 1)\n",
      "🔍 Starting monitoring workflow for: https://ain-rhone.msa.fr/lfp/soutien-exploitant\n",
      "📥 Fetching content from: https://ain-rhone.msa.fr/lfp/soutien-exploitant (attempt 1)\n",
      "🔍 Starting monitoring workflow for: https://agriculture.gouv.fr/mots-cles/aides\n",
      "📥 Fetching content from: https://agriculture.gouv.fr/mots-cles/aides (attempt 1)\n"
     ]
    }
   ],
   "source": [
    "# Run 1\n",
    "dataset = _langfuse_client.get_dataset(\"filtered-article-dataset\")\n",
    "for task in list_tasks:\n",
    "    print(f\"Running experiment for workflow: {task.__name__}\")\n",
    "    result = dataset.run_experiment(\n",
    "        name=\"List article experiment with \" + task.__name__,\n",
    "        description=f\"Testing {task.__name__} for article listing task\",\n",
    "        task=task,\n",
    "        evaluators=[recall_evaluator, precision_evaluator],\n",
    "        max_concurrency=5\n",
    "    )\n",
    "\n",
    "print(result.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc88d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agricultural_monitoring.workflows.agent_workflow import WebAgentStandaloneProcessor\n",
    "\n",
    "web_agent_processor = WebAgentStandaloneProcessor()\n",
    "result = await web_agent_processor.process_content(\"https://www.franceagrimer.fr/rechercher-une-aide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"aides_identifiees\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agricultural_monitoring.workflows.agent_workflow import DataNormalizer\n",
    "\n",
    "data_norm = DataNormalizer()\n",
    "new_result = data_norm.normalize_data(result, \"https://www.franceagrimer.fr/rechercher-une-aide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f3ef7",
   "metadata": {},
   "source": [
    "## 2. Quick Start - Complete Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced workflow with inline link processing\n",
    "workflow = EnhancedAgriculturalMonitoringWorkflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc68b6f",
   "metadata": {},
   "source": [
    "## 3. Test Single URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with one URL first\n",
    "test_url = TARGET_URLS[2]\n",
    "print(f\"🧪 Testing workflow with: {test_url}\")\n",
    "\n",
    "result = await workflow.monitor_url(test_url)\n",
    "\n",
    "print(f\"\\n📊 Results:\")\n",
    "print(f\"Web extraction: {result['web_content']['status']}\")\n",
    "print(f\"LLM processing: {result['llm_extraction']['status']}\")\n",
    "print(f\"Data normalization: {result['normalized_data']['metadata']['status']}\")\n",
    "print(f\"Memory filtering: {result.get('memory_filtered_data', {}).get('status', 'N/A')}\")\n",
    "\n",
    "# Show found aids\n",
    "memory_data = result.get('memory_filtered_data', {})\n",
    "if memory_data.get('status') == 'success':\n",
    "    aids_found = memory_data.get('aides', [])\n",
    "    before_filtering = len(result['normalized_data']['aides'])\n",
    "    after_filtering = len(aids_found)\n",
    "    \n",
    "    print(f\"\\n🔍 Filtering results:\")\n",
    "    print(f\"   Before filtering: {before_filtering} aids\")\n",
    "    print(f\"   After filtering: {after_filtering} aids\")\n",
    "    print(f\"   Filtered out: {memory_data.get('filtered_count', 0)} aids\")\n",
    "    \n",
    "    if aids_found:\n",
    "        print(f\"\\n✅ New agricultural aids found:\")\n",
    "        for i, aide in enumerate(aids_found[:3], 1):\n",
    "            print(f\"   {i}. {aide['titre_aide']}\")\n",
    "            print(f\"      {aide['description'][:100]}...\")\n",
    "    else:\n",
    "        print(f\"\\n📝 No new aids found (all filtered by memory)\")\n",
    "else:\n",
    "    print(f\"\\n❌ Workflow failed at some stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9747f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset.items:\n",
    "    print(item.input, item.expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset.items:\n",
    "    print(item.input, item.expected_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb7288",
   "metadata": {},
   "source": [
    "## 4. Production Monitoring - All URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcaf283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run production monitoring with all URLs\n",
    "print(\"🚀 Running production monitoring with all target URLs...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the monitoring function for comprehensive tracking\n",
    "results = run_monitored_workflow(workflow, TARGET_URLS)\n",
    "\n",
    "# Detailed results analysis\n",
    "successful_results = results['successful_results']\n",
    "failed_results = results['failed_results']\n",
    "summary = results['summary']\n",
    "\n",
    "print(f\"\\n📋 DETAILED RESULTS:\")\n",
    "for i, result in enumerate(successful_results + failed_results, 1):\n",
    "    url = result.get('web_content', {}).get('url', 'Unknown')\n",
    "    status = result.get('normalized_data', {}).get('metadata', {}).get('status', 'Unknown')\n",
    "    \n",
    "    print(f\"\\n{i}. {url}\")\n",
    "    print(f\"   Status: {status}\")\n",
    "    \n",
    "    if status == 'success':\n",
    "        aids_before = len(result['normalized_data']['aides'])\n",
    "        memory_data = result.get('memory_filtered_data', {})\n",
    "        aids_after = memory_data.get('new_count', aids_before) if memory_data.get('status') == 'success' else aids_before\n",
    "        \n",
    "        print(f\"   Web extraction: ✅\")\n",
    "        print(f\"   Content length: {len(result['web_content']['content']):,} chars\")\n",
    "        print(f\"   Links found: {result['web_content'].get('links_count', 0)}\")\n",
    "        print(f\"   Aids before filtering: {aids_before}\")\n",
    "        print(f\"   Aids after filtering: {aids_after}\")\n",
    "        \n",
    "        # Show sample aids\n",
    "        final_aids = memory_data.get('aides', result['normalized_data']['aides']) if memory_data.get('status') == 'success' else result['normalized_data']['aides']\n",
    "        if final_aids:\n",
    "            print(f\"   Sample aids:\")\n",
    "            for aide in final_aids[:2]:\n",
    "                print(f\"     • {aide['titre_aide']}\")\n",
    "    else:\n",
    "        error = result.get('normalized_data', {}).get('metadata', {}).get('error', 'Unknown error')\n",
    "        print(f\"   Error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687fb77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agricultural_monitoring.extractors import EnhancedWebContentExtractor\n",
    "from agricultural_monitoring.processors import LLMProcessor, WebAgentProcessor\n",
    "from agricultural_monitoring.processors import DataNormalizer\n",
    "\n",
    "URL = \"https://www.bretagne.bzh/aides/?mot-clef=&profil=entreprises-et-professionnels&cloture=0&showall=0\"\n",
    "\n",
    "web_extractor = EnhancedWebContentExtractor()\n",
    "llm_processor = LLMProcessor()\n",
    "web_agent_processor = WebAgentProcessor()\n",
    "normalizer = DataNormalizer()\n",
    "\n",
    "# Step 2: Process the extracted content\n",
    "processed_content = await web_agent_processor.process_content(URL)\n",
    "\n",
    "# Step 3: Normalize the processed content\n",
    "normalized_content = normalizer.normalize_data(processed_content, URL)\n",
    "\n",
    "print(processed_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"test.json\", \"w\") as f:\n",
    "    json.dump(normalized_content, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d5d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# 1. Take screenshot\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.franceagrimer.fr/rechercher-une-aide\")\n",
    "driver.save_screenshot(\"page.png\")\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aides-agri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
