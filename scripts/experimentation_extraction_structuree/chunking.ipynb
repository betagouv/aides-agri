{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "langfuse = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae1c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# List dataset files, open each JSON file (each contains a list of dicts), and iterate items\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 1. Directory containing the dataset JSON files (relative to repo root / notebook)\n",
    "dataset_dir = os.path.join(\"datasets\")\n",
    "\n",
    "# 1. Create a list containing all file names from datasets folder\n",
    "dataset_files = sorted([fn for fn in os.listdir(dataset_dir) if fn.endswith('.json')])\n",
    "print(f\"Found {len(dataset_files)} dataset files:\")\n",
    "for fn in dataset_files:\n",
    "    print(f\" - {fn}\")\n",
    "\n",
    "# 2. Open and read each file. Each file is expected to contain a list of dicts.\n",
    "all_data = {}\n",
    "for fname in dataset_files:\n",
    "    path = os.path.join(dataset_dir, fname)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        print(f\"Warning: file {fname} does not contain a list (got {type(data)}). Skipping.\")\n",
    "        continue\n",
    "\n",
    "    all_data[fname] = data\n",
    "\n",
    "    # 3. Iterate over dict items\n",
    "    print(f\"\\nFile: {fname} contains {len(data)} items. Showing up to first 3 items:\")\n",
    "    for i, item in enumerate(data[:3]):\n",
    "        if isinstance(item, dict):\n",
    "            keys = list(item.keys())\n",
    "            # Short preview of values (truncate long strings)\n",
    "            preview = json.dumps(item, ensure_ascii=False)\n",
    "            if len(preview) > 300:\n",
    "                preview = preview[:300] + ' ...[truncated]'\n",
    "            print(f\"  Item {i}: keys={keys}\")\n",
    "            print(f\"    preview: {preview}\")\n",
    "        else:\n",
    "            print(f\"  Item {i}: not a dict (type={type(item)}) -> {item}\")\n",
    "\n",
    "print(\"\\nAll files loaded into variable 'all_data' (mapping filename -> list of items).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_extraction.adapters.albert_structured_extractor import AlbertStructuredExtractor\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Categorie(BaseModel):\n",
    "    categorie : str = Field(title=\"Catégorie de classification\")\n",
    "\n",
    "llm = AlbertStructuredExtractor(Categorie)\n",
    "\n",
    "# Get production prompt\n",
    "PROMPT = langfuse.get_prompt(\"Classification\", label=\"latest\")\n",
    "file_name = \"hierarchical_slice1.json\"\n",
    "\n",
    "accuracy = 0\n",
    "fn_autre = 0\n",
    "preds = []\n",
    "labels = []\n",
    "for chunk in tqdm(all_data[file_name]):\n",
    "    titre = chunk[\"input\"][\"title\"]\n",
    "    current_chunk = chunk[\"input\"][\"chunk\"]\n",
    "    next_chunks = chunk[\"input\"][\"next_chunks\"]\n",
    "    previous_chunks = chunk[\"input\"][\"previous_chunks\"]\n",
    "    full_prompt = PROMPT.compile(\n",
    "        title=titre,\n",
    "        previous_chunks=previous_chunks,\n",
    "        chunk=current_chunk,\n",
    "        next_chunks=next_chunks\n",
    "    )\n",
    "    label = chunk[\"expected_output\"]\n",
    "    answer = llm.get_structured_output(\"albert-small\", system_prompt=full_prompt[0][\"content\"], user_message=full_prompt[1][\"content\"], temperature=0.2)\n",
    "    json_answer = answer.get_json()\n",
    "    preds.append(json_answer[\"categorie\"])\n",
    "    labels.append(label)\n",
    "    if label == \"autre\" and json_answer[\"categorie\"] != \"autre\":\n",
    "        fn_autre += 1\n",
    "    if json_answer[\"categorie\"] == label:\n",
    "        accuracy += 1\n",
    "print(f\"Accuracy: {accuracy/len(all_data[file_name])*100:.2f}%\")\n",
    "print(f\"False negatives (autre): {fn_autre/len(all_data[file_name])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85457fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"False negatives (autre): {fn_autre/len(all_data[file_name])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab46ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prompt = PROMPT.compile(\n",
    "        title=\"titre\",\n",
    "        previous_chunks=\"previous_chunks\",\n",
    "        chunk=\"current_chunk\",\n",
    "        next_chunks=\"next_chunks\"\n",
    "    )\n",
    "full_prompt[0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d7837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ff26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = langfuse.get_prompt(\"Classification\", label=\"latest\")\n",
    "test_dict = {\n",
    "    \"chunk\": \"chunk\",\n",
    "    \"previous_chunks\": \"prev_cunk\",\n",
    "    \"next_chunks\": \"next_chunk\",\n",
    "    \"title\": \"titre\"\n",
    "}\n",
    "compiled_prompt = PROMPT.compile(**test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_extraction.adapters.hierarchical_file_chunker import HierarchicalFileChunker\n",
    "\n",
    "all_chunks = []\n",
    "chunker = HierarchicalFileChunker(separators=[\"##### \", \"#### \", \"### \", \"## \", \"#\"])\n",
    "for file_path in [\"sample_pdf/agence_eau.pdf\", \"sample_pdf/reinsertion_professionnelle.pdf\", \"sample_pdf/relance_exploitation.pdf\"]:\n",
    "    chunks = chunker.chunk_file(file_path)\n",
    "    all_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "eval_chunks = []\n",
    "slice_size = 5\n",
    "no_chunk_msg = \"No chunk is available.\"\n",
    "\n",
    "with open(\"datasets/base_parser.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for i, chunk_obj in enumerate(all_chunks):\n",
    "    # Extract text content for easier handling\n",
    "    chunk_text = chunk_obj.get(\"content\", \"\")\n",
    "\n",
    "    # Previous chunks\n",
    "    if i == 0:\n",
    "        prev_chunks = no_chunk_msg\n",
    "    else:\n",
    "        prev_chunks = [all_chunks[j][\"content\"] for j in range(max(0, i - slice_size), i)]\n",
    "\n",
    "    # Next chunks\n",
    "    if i >= len(all_chunks) - 1:\n",
    "        next_chunks = no_chunk_msg\n",
    "    else:\n",
    "        next_chunks = [all_chunks[j][\"content\"] for j in range(i + 1, min(i + 1 + slice_size, len(all_chunks)))]\n",
    "\n",
    "    eval_chunks.append(\n",
    "        {\n",
    "            \"input\": \n",
    "            {\n",
    "                \"chunk\": chunk_text,\n",
    "                \"previous_chunks\": \"\\n\\n\".join(prev_chunks) if isinstance(prev_chunks, list) else prev_chunks,\n",
    "                \"next_chunks\": \"\\n\\n\".join(next_chunks) if isinstance(next_chunks, list) else next_chunks,\n",
    "                \"title\": chunk_obj[\"title\"]\n",
    "            },\n",
    "            \"expected_output\": data[i][\"expected_output\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 3. Réécrire la liste mise à jour dans le fichier\n",
    "with open(\"fruits.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_chunks, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Items ajoutés avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"ALBERT_API_KEY\"),\n",
    "    base_url=\"https://albert.api.etalab.gouv.fr/v1\"\n",
    ")\n",
    "\n",
    "PROMPT = langfuse.get_prompt(\"Classification\", label=\"latest\")\n",
    "\n",
    "# Define your task function\n",
    "def my_task(*, item, **kwargs):\n",
    "    context = item[\"input\"]\n",
    "    compiled_prompt = PROMPT.compile(**context)\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"ALBERT_API_KEY\"),\n",
    "        base_url=\"https://albert.api.etalab.gouv.fr/v1\"\n",
    "    )\n",
    "\n",
    "    print(compiled_prompt)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"albert-large\", \n",
    "        messages=compiled_prompt\n",
    "    )\n",
    " \n",
    "    return {\"output\": response.choices[0].message.content}\n",
    "\n",
    "# Run experiment on local data\n",
    "local_data = [\n",
    "    {\n",
    "        \"input\": \n",
    "            {\n",
    "                \"chunk\": \"Il.1. Nature et durée de l'activité du demandeur\\n\\nPour bénéficier du dispositif de l'aide à la réinsertion professionnelle, le demandeur doit justifier à la date de dépôt du dossier de 5 années d'activité agricole au sens de l'art. L. 311-1 du code rural et de la pêche maritime! (sont cependant exclues les activités aquacoles et équestres), précédant-mmédiatementde-dépôt dea-demande-d'ARP. en qualité de :\\n\\n- ° exploitant agricole ou associé exploitant, à titre principal, affilié à l'assurance maladie, invalidité, maternité des personnes non-salariées des professions agricoles (AMEXA), ou\\n- ° conjoint de chef d'exploitation à titre principal participant aux travaux ou de conjoint collaborateur, bénéficiant à ce titre de l'AMEXA, ou\\n- ° aide familial bénéficiant de l'AMEXA.\",\n",
    "                \"previous_chunks\": \"## IL Conditions d'éligibilité du demandeur\",\n",
    "                \"next_chunks\": \"## 11.2. Engagements du demandeur\\n\\nLe bénéficiaire de l'aide à la réinsertion professionnelle :\\n\\n- ° doit s'engager à ne pas revenir à l'agriculture en qualité de chef d'exploitation ou d'entreprise agricole, de conjoint ou d'aide ( cf. 11.1) pendant une durée de 5 ans à compter de l'attribution de l'aide (date de la décision préfectorale d'octroi de l'aide) ;\\n- ° peut toutefois conserver une parcelle de subsistance qui ne doit pas excéder un hectare de surface agricole utile pondérée (SAUP) ;\\n- e ne doit pas être à deux ans de l'âge légal de la retraite, ou à la retraite à la date de dépôt du dossier.\\n\\n1 L'artide L. 311-1 du code rural dispose que : « Sont réputées agricoles toutes les activités correspondant à la maîtrise d'un cycle biologique de caractère végétal ou animal et constituant une ou plusieurs étapes nécessaires au déroulement de ce cycle ainsi que les activités exercées par un exploitant agricole qui sont dans le prolongement de l'acte de production ou qui ont pour support l'exploitation. Les activités marines sont réputées agricoles, nonobstant le statut social dont relèvent ceux qui les pratiquent. Il en est de même des activités de préparation et d'entraînement des équidés domestiques en vue de leur exploitation, à l'exclusion des activités de spectacle.\",\n",
    "                \"title\": \"IL Conditions d'éligibilité du demandeur\"\n",
    "            },\n",
    "        \"expected_output\": \"eligibilite\"\n",
    "    },\n",
    "]\n",
    "\n",
    "result = langfuse.run_experiment(\n",
    "    name=\"Geography Quiz\",\n",
    "    description=\"Testing basic functionality\",\n",
    "    data=local_data,\n",
    "    task=my_task,\n",
    ")\n",
    "\n",
    "print(result.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cad45b",
   "metadata": {},
   "source": [
    "# Run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display chunks with their titles to show document structure\n",
    "print(f\"Total chunks: {len(new_chunks)}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, chunk in enumerate(new_chunks[:10], 1):  # Show first 10 chunks\n",
    "    title = chunk.get('title', 'No title')\n",
    "    content_preview = chunk['content'][:100].replace('\\n', ' ')\n",
    "    \n",
    "    print(f\"\\nChunk {i}\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Content preview: {content_preview}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6210a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b3246",
   "metadata": {},
   "source": [
    "# Chunk Classification with LLM\n",
    "\n",
    "Classification of chunks into predefined categories based on the dispositif d'aide schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "# Define classification categories based on schema_dispositif_aide\n",
    "class ChunkCategory(str, Enum):\n",
    "    \"\"\"Categories for chunk classification based on dispositif aide schema\"\"\"\n",
    "    PRESENTATION_AIDE = \"presentation_aide\"  # Title and description\n",
    "    ELIGIBILITE = \"eligibilite\"  # Who can access, eligible projects\n",
    "    TYPE_AIDE = \"type_aide\"  # Types of financial aid\n",
    "    PORTEURS = \"porteurs\"  # Actors involved in implementation\n",
    "    INFORMATIONS_EXTERNES = \"informations_externes\"  # External links and parent programs\n",
    "    BENEFICIAIRES = \"beneficiaires\"  # Target beneficiaries\n",
    "    ELIGIBILITE_GEOGRAPHIQUE = \"eligibilite_geographique\"  # Geographic coverage\n",
    "    DATES = \"dates\"  # Opening and closing dates\n",
    "    OPERATIONS_ELIGIBLES = \"operations_eligibles\"  # Eligible operations/expenses\n",
    "    CADRE_LEGAL = \"cadre_legal\"  # Legal framework and regulations\n",
    "    AUTRE = \"autre\"  # Other/Uncategorized\n",
    "\n",
    "\n",
    "class ClassifiedChunk(BaseModel):\n",
    "    \"\"\"Schema for a classified chunk\"\"\"\n",
    "    chunk_id: str = Field(..., description=\"ID of the chunk\")\n",
    "    content: str = Field(..., description=\"Content of the chunk\")\n",
    "    category: ChunkCategory = Field(..., description=\"Classified category\")\n",
    "    confidence: float = Field(..., description=\"Confidence score (0-1)\", ge=0, le=1)\n",
    "    reasoning: Optional[str] = Field(None, description=\"Explanation for the classification\")\n",
    "\n",
    "\n",
    "class ChunkClassificationResult(BaseModel):\n",
    "    \"\"\"Schema for classification results\"\"\"\n",
    "    classifications: List[ClassifiedChunk] = Field(..., description=\"List of classified chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49283f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-test: synthetic labels (small), build and export matrix to local files (in notebook folder)\n",
    "true = ['eligibilite','presentation_aide','operations_eligibles','eligibilite','autre','operations_eligibles','autre']\n",
    "                \n",
    "pred = ['eligibilite','presentation_aide','autre','eligibilite','autre','operations_eligibles','presentation_aide']\n",
    "labels = ['presentation_aide','eligibilite','operations_eligibles','autre']\n",
    "cm = build_confusion_matrix(true, pred, labels=labels)\n",
    "print('Confusion matrix (counts):')\n",
    "print(cm)\n",
    "\n",
    "# Export CSV and PNG (png will be shown inline as well)\n",
    "export_confusion_matrix(cm, path_csv='confusion_matrix_counts.csv', path_png='confusion_matrix_heatmap.png', normalize=True)\n",
    "print('Exported files: confusion_matrix_counts.csv, confusion_matrix_heatmap.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aides-agri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
